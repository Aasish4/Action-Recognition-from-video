# -*- coding: utf-8 -*-
"""Action_recogniztion_video.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L54MJr5awBy59lqorEzKphl4D0h6qWo1

# Data Preparation
"""

# Import all the necessary dependendicies
import pandas as pd
import joblib
import os
import numpy as np
from tqdm import tqdm
from sklearn.preprocessing import LabelBinarizer # for binarizing the label catagories

import torch
import torch.nn as nn 
import torch.nn.functional as F 
import joblib

import albumentations # for image augmentation
import torch.optim as optim 
import matplotlib.pyplot as plt
import time
plt.style.use('ggplot')

from imutils import paths
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from PIL import Image

import cv2

# List all the folders
all_paths = os.listdir('/content/drive/MyDrive/My project/MY PROJECTS/Video classification/data')
all_paths

# Check the number of classes for training
folder_paths = [x for x in all_paths if os.path.isdir('/content/drive/MyDrive/My project/MY PROJECTS/Video classification/data/'+ x)]
print(f'Folder paths: {folder_paths}')
print(f'Number of folders: {len(folder_paths)}')

create_labels = ['chess', 'cricket', 'football', 'swimming']
data =  pd.DataFrame()

# Extract the image path and labels into a dataframe
image_formats = ['jpg', 'JPG', 'PNG', 'png']
labels = []
counter = 0
for i, folder_path in tqdm(enumerate(folder_paths), total=len(folder_paths)):
  if folder_path not in create_labels:
    continue
  image_paths = os.listdir('/content/drive/MyDrive/My project/MY PROJECTS/Video classification/data/'+folder_path)
  label = folder_path
  for image_path in image_paths:
    if image_path.split('.')[-1] in image_formats:
      data.loc[counter, 'image_path'] = f'/content/drive/MyDrive/My project/MY PROJECTS/Video classification/data/{folder_path}/{image_path}'
      labels.append(folder_path)
      counter += 1

# Convert the labels into onehot
labels = np.array(labels)
lb = LabelBinarizer()
labels = lb.fit_transform(labels)

labels[0]

if len(labels[0]) ==1:
  for i in range(len(labels)):
    index = labels[i]
    data.loc[i, 'target'] = int(index)
elif len(labels[0]) > 1:
  for i in range(len(labels)):
      index = np.argmax(labels[i])
      data.loc[i, 'target'] = int(index)

data.head()

data = data.sample(frac=1).reset_index(drop=True)

print(f'Number of labels or classes: {len(lb.classes_)}')
print(f'The first one hot encoded labels: {labels[0]}')
print(f'Mapping the category: {lb.classes_[0]}')
print(f'Total instances: {len(data)}')

# Convert the dataframe to csv
data.to_csv('data.csv', index=False)

print('Saving the binarized labels as pickled file')
joblib.dump(lb, 'lb.pkl')

print(data.head(10))

# create a symbolic link in google drive
!ln -s '/content/drive/MyDrive/My project/MY PROJECTS/Video classification/data' '/content/data'

cd '/content/data'

"""# Constructing the deep neural network"""

# Load the binarized label file
lb = joblib.load('lb.pkl')

# Define the neural network
class CustomCNN(nn.Module):
  def __init__(self):
    super(CustomCNN, self).__init__()
    self.conv1 = nn.Conv2d(3, 16, 5)
    self.conv2 = nn.Conv2d(16, 32, 5)
    self.conv3 = nn.Conv2d(32, 64, 3)
    self.conv4 = nn.Conv2d(64, 128, 5)

    self.fc1 = nn.Linear(128, 256)
    self.fc2 = nn.Linear(256, len(lb.classes_)) # lb.classes_ indicates the number of classes

    self.pool = nn.MaxPool2d(2,2)
  def forward(self, x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    x = self.pool(F.relu(self.conv4(x)))
    bs,_,_,_ = x.shape
    x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)
    x = F.relu(self.fc1(x))
    x = self.fc2(x)
    return x

lr = 1e-3  # learning rate
batch_size = 32

device = 'cuda:0'
print(f'Computation device: {device}\n')

df = pd.read_csv('data.csv')
X = df.image_path.values
y = df.target.values

# Split the data into training and testing set
(xtrain, xtest, ytrain, ytest) = train_test_split(X,y, test_size=0.10, random_state=42)

print(f'Training instances: {len(xtrain)}')
print(f'Validation instances: {len(xtest)}')

# Construct a dataset with image augmentation applied
class ImageDataset(Dataset):
  def __init__(self, images, labels=None, tfms=None):
    self.X = images
    self.y = labels

    # Apply the augmentations
    if tfms == 0: # for validation set
      self.aug = albumentations.Compose([
                                         albumentations.Resize(224, 224, always_apply=True)
      ])
    else: # for training set
      self.aug = albumentations.Compose([
                                         albumentations.Resize(224, 224, always_apply=True),
                                         albumentations.HorizontalFlip(p=0.5),
                                         albumentations.ShiftScaleRotate(
                                             shift_limit=0.3,
                                             scale_limit=0.3,
                                             rotate_limit=15,
                                             p=0.5
                                         )
      ])
  
  def __len__(self):
    return (len(self.X))
  
  def __getitem__(self, i):
    image = Image.open(self.X[i])
    image = image.convert('RGB')
    image = self.aug(image=np.array(image))['image']
    image = np.transpose(image, (2,0,1)).astype(np.float32)
    label = self.y[i]
    return (torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long))

train_data = ImageDataset(xtrain, ytrain, tfms=1)
test_data = ImageDataset(xtest,ytest, tfms=0)

# Iterate through data present inside using DataLoader
trainloader = DataLoader(train_data, batch_size = batch_size, shuffle=True)
testloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

# Initilizing the NN model in gpu
model = CustomCNN().to(device)
print(model)

total_params = sum(p.numel() for p in model.parameters())
print(f'{total_params:} total parameters')
total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f'{total_trainable_params:} trainable paramaters')

optimizer = optim.Adam(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()

scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    patience=5,
    factor=0.5,
    min_lr=1e-6,
    verbose=True
)

# Training function
def fit(model, train_dataloader):
    print('Training')
    model.train()
    train_running_loss = 0.0
    train_running_correct = 0
    for i, data in tqdm(enumerate(trainloader), total=int(len(train_data)/trainloader.batch_size)):
        data, target = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, target)
        train_running_loss += loss.item()
        _, preds = torch.max(outputs.data, 1)
        train_running_correct += (preds == target).sum().item()
        loss.backward()
        optimizer.step()
        
    train_loss = train_running_loss/len(trainloader.dataset)
    train_accuracy = 100. * train_running_correct/len(trainloader.dataset)
    
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}")
    
    return train_loss, train_accuracy

# Validation function
def validate(model, testloader):
    print('Validating')
    model.eval()
    val_running_loss = 0.0
    val_running_correct = 0
    with torch.no_grad():
        for i, data in tqdm(enumerate(testloader), total=int(len(test_data)/testloader.batch_size)):
            data, target = data[0].to(device), data[1].to(device)
            outputs = model(data)
            loss = criterion(outputs, target)
            
            val_running_loss += loss.item()
            _, preds = torch.max(outputs.data, 1)
            val_running_correct += (preds == target).sum().item()
        
        val_loss = val_running_loss/len(testloader.dataset)
        val_accuracy = 100. * val_running_correct/len(testloader.dataset)
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')
        
        return val_loss, val_accuracy

# Training and validating the model 
train_loss , train_accuracy = [], []
val_loss , val_accuracy = [], []
epoches = 75
start = time.time()
for epoch in range(epoches):
    print(f"Epoch {epoch+1} of {epoches}")
    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)
    val_epoch_loss, val_epoch_accuracy = validate(model, testloader)
    train_loss.append(train_epoch_loss)
    train_accuracy.append(train_epoch_accuracy)
    val_loss.append(val_epoch_loss)
    val_accuracy.append(val_epoch_accuracy)
    scheduler.step(val_epoch_loss)
end = time.time()
print(f"{(end-start)/60:.3f} minutes")

# accuracy plots
plt.figure(figsize=(10, 7))
plt.plot(train_accuracy, color='green', label='train accuracy')
plt.plot(val_accuracy, color='blue', label='validataion accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracy.png')
plt.show()
# loss plots
plt.figure(figsize=(10, 7))
plt.plot(train_loss, color='orange', label='train loss')
plt.plot(val_loss, color='red', label='validataion loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss.png')
plt.show()
    
# serialize the model to disk
print('Saving model...')
torch.save(model.state_dict(), 'video_classif.pth')
 
print('TRAINING COMPLETE')

"""# Tesing the model"""

print('Loading the model and label binarizer..')
lb = joblib.load('lb.pkl')
model = CustomCNN().cuda()

model.load_state_dict(torch.load('video_classif.pth'))
print('Loaded model state_dict...')

# for augmentation we only resize the image to 224*224
aug = albumentations.Compose([albumentations.Resize(224,224)])

# loading the video
cap = cv2.VideoCapture('Video Of  Man Swimming.mp4')
if (cap.isOpened() == False):
  print('Error while trying to read video')
else:
  print('Video loaded..')

frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
print(f'Video dimention is: {frame_width}*{frame_height}')

# definne the codec and create the video writer object
out = cv2.VideoWriter('Output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width,frame_height))

# read until end of video
while(cap.isOpened()):
    # capture each frame of the video
    ret, frame = cap.read()
    if ret == True:
        model.eval()
        with torch.no_grad():
            # conver to PIL RGB format before predictions
            pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            pil_image = aug(image=np.array(pil_image))['image']
            pil_image = np.transpose(pil_image, (2, 0, 1)).astype(np.float32)
            pil_image = torch.tensor(pil_image, dtype=torch.float).cuda()
            pil_image = pil_image.unsqueeze(0)
            
            outputs = model(pil_image)
            _, preds = torch.max(outputs.data, 1)
        
        cv2.putText(frame, lb.classes_[preds], (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 200, 0), 2)
        #cv2.imshow('image', frame)
        out.write(frame)
        # press `q` to exit
        if cv2.waitKey(27) & 0xFF == ord('q'):
            break
    else: 
        break
# release VideoCapture()
cap.release()
# close all frames and video windows
cv2.destroyAllWindows()